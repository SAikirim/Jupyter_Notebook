{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공 신경망(ANN)\n",
    "## 다층 퍼셉트론(Multi-Layer Perceptron, MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퍼셉트론\n",
    "* 입력을 받아서 신호로 출력\n",
    "* 퍼셉트론은 직선으로 나뉜 두 영역을 만듦\n",
    "    - AND\n",
    "    - OR\n",
    "* 2개의 계층을 만들어 XOR 계산 가능\n",
    "    - XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 0, 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def AND(x1, x2):\n",
    "    X = np.array([x1, x2])\n",
    "    W = np.array([0.5, 0.5])\n",
    "    b = -0.6\n",
    "    tmp = np.sum(W*X)+b\n",
    "    if tmp <= 0 :\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "AND(1,1), AND(1,0), AND(0,1), AND(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def OR(x1, x2):\n",
    "    X = np.array([x1, x2])\n",
    "    W = np.array([0.5, 0.5])\n",
    "    b = -0.3\n",
    "    tmp = np.sum(W*X)+b\n",
    "    if tmp <= 0 :\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "OR(1,1), OR(1,0), OR(0,1), OR(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 1, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NAND(x1, x2):\n",
    "    X = np.array([x1, x2])\n",
    "    W = np.array([-0.5, -0.5])\n",
    "    b = 0.6\n",
    "    tmp = np.sum(W*X)+b\n",
    "    if tmp <= 0 :\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "NAND(1,1), NAND(1,0), NAND(0,1), NAND(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 1, 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def XOR(x1, x2):\n",
    "    S1 = NAND(x1,x2)\n",
    "    S2 = OR(x1,x2)\n",
    "    return AND(S1, S2)\n",
    "\n",
    "XOR(1,1), XOR(1,0), XOR(0,1), XOR(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망\n",
    "* 입력층\n",
    "* 은닉층\n",
    "    - 사람 눈에는 보이지 않는다.\n",
    "    - 많이 존재할 수 있으며, 싶어지면 딥러닝이라 한다.\n",
    "* 출력층\n",
    "* 뉴런(노드)\n",
    "\n",
    "### 신경망은 데이터를 통해 학습할 수 있다.\n",
    "* 데이터에서 _학습_한다는 것은 '가중치' 매개 변수의 값을 데이터를 통해 자동으로 결정한다는 뜻\n",
    "* 자동으로 결정된다는 것은 수작업으로 매개변수 값을 설정할 필요가 없다는 뜻임\n",
    "    - 편향도 자동으로 결정됨, 큰의미에서는 입력값이 1인 가중치라고 생각해도 됨\n",
    "\n",
    "\n",
    "## 활성화 함수\n",
    "* 임계값을 경계로 출력이 바뀜\n",
    "* Activattion Funtion에 미분함 왜?(오차역전파법을 이용해 가중치를 업데이트 하기 위해)\n",
    "    - Liner function : 사용하는 의미가 없다.\n",
    "        + 점수 -> 점수\n",
    "    - 계단함수(Step function)\n",
    "        + 0, 1로 출력(입력이 0을 넘기면 1, 이외에는 0)\n",
    "    - 시그모이드 함수(Sigmoid function)\n",
    "        + ${\\displaystyle S(x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{e^{x}+1}}.}$\n",
    "        + 로지스틱 함수?\n",
    "        + 결과값을 0 ~ 1사이로 나타냄\n",
    "        + 점수 -> '확률'로 출력함\n",
    "        + 초기에는 많이 사용하였지만\n",
    "            * __기울기 소실(gradient vanishing) 문제가 발생함__\n",
    "            * 일정값 이상의 input을 입력하면 미분값이 0에 가까워진다.\n",
    "        + __함수의 중심값이 0이 아니다.__\n",
    "            * 학습의 속도가 느려질 수 있다.\n",
    "    - Tanh\n",
    "        + $tanh(x)=ex−e−xex+e−x$\n",
    "        + 대칭형\n",
    "        + -1 ~ 1로 나타냄\n",
    "        + sigmoid처럼 범위 안의 값을 예측할 때 사용\n",
    "        + __함수의 중심값이 0이 아닌 문제를 해결__\n",
    "        + __기울기 소실 문제는 계속 남아있다.__\n",
    "    - ReLU\n",
    "        + $f(x) = max(0, x)$\n",
    "        + 0 보다 작으면 0으로, 0보다 크면 입력값 그대로 출력\n",
    "        + 출력값이 항상 양수여야 한다면 사용\n",
    "        + __학습이 빠르다.__\n",
    "        + __구현이 쉽다.__\n",
    "        + __음수의 node들은 학습이 안될 수도 있다.__\n",
    "        + 가장 많이 쓰임\n",
    "    - Leakly ReLU\n",
    "        + $f(x)=max(0.01x,x)$\n",
    "        + 음수 값도 조금씩이라도 학습이 된다.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수\n",
    "* 신경망 성능의 '나쁨'을 나타내는 지표\n",
    "    - 비용함수(Cost Function)이라고도 불린다.\n",
    "    - 손실에는 그만큼 비용이 발생하기 때문\n",
    "    - loss function : 개별적\n",
    "    - Cost Function : 1/m (평균을 냄)\n",
    "* 손실함수 종류\n",
    "    - 평균 제곱 오차(MSE, Mean Squared Error)\n",
    "        + $MSE = {\\dfrac {1}{m}}\\sum_{i=1}^{n}(Y_{i}-{b_{i}})^{2}$\n",
    "    - 교차 엔트로피 오차(Cross Entropy)\n",
    "        + 필요없는 정보를 제거\n",
    "        \n",
    "* 손실함수를 사용하는 이유?\n",
    "    - 우리의 목적은 높은 '정확도'를 끌어내는 매개변수 값을 찾는 것!\n",
    "    - '정확도'라는 실제적인 값이 있지만, 이를 사용하지 않고 함수를 사용하는 이유는 __미분을 통해 최적 값을 찾아낼수 있기 때문__\n",
    "        + 미분(기울기)을 계산을 반복, 미분값이 0이되면 최적값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tenserflow\n",
    "* 고수준 미분기\n",
    "\n",
    "## 케라스\n",
    "* 모든 종류의 신경망을 손쉽게 만들고 훈련 평가, 실행할 수 있는 고수준 딥러닝 API\n",
    "\n",
    "# tensorflow 설치\n",
    "* gpu 드라이버(최신)\n",
    "* cuda(10.1)\n",
    "* cuDNN(10.1)\n",
    "* pip로 tensorflow(2.3) 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-87e201d3fc0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 실습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# 실습 문제\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "def make_random_data():\n",
    "    x = np.random.uniform(low=-2, high=2, size=200)\n",
    "    y = []\n",
    "    for t in x:\n",
    "        r = np.random.normal(loc=0.0, scale=(0.5 + t*t/3), size=None)\n",
    "        y.append(r)\n",
    "    return  x, 1.726*x -0.84 + np.array(y)\n",
    "\n",
    "\n",
    "x, y = make_random_data() \n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 실습 문제\n",
    "1. 데이터 분리 안해도 됨\n",
    "2. Sequential\n",
    "3. Layer???\n",
    "4. optimizer 'sgd'\n",
    "5. loss = 'mse' # 회귀 모뎅에서 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 알고리즘 구현\n",
    "* Goal : 가중치와 편향을 훈련데이터에 맞게 update\n",
    "    1. 데이터 입력(미니배치)\n",
    "    2. 신경망을 지나고 손실함수에 대한 loss 값을 계산\n",
    "    3. 손실함수에 대한 미분값을 계산 ,| __activation function 사용__\n",
    "    4. 경사하강법 사용 -> update(가중치 매개변수 갱신)\n",
    "    => 수치 미분(계산이 오래걸림)     |=> 오차역전파법(activation function 사용)\n",
    "    5. 최적값 구할때까지 2,3,4 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오차역전파법\n",
    "* 오차를 역으로 전파시켜준다. (역으로 보정해준다.)\n",
    "* 오차 : 실제값 - 예측값\n",
    "* Layer -> layer -> layer -> ... -> 결과\n",
    "* Layer오차 <- layer오차 -> layer오차 <- ... <- 결과의 오차\n",
    "* 경사하강법을 이용해 각각의 오차를 보정해 준다.\n",
    "* 경사하강법\n",
    "    - 미분값을 구함 -> 수치미분 -> 컴퓨팅 파워가 많이 필요함\n",
    "    - 미분값을 구함 -> Chain Rule을 이용해 구한다. -> 각 function의 미분 결과를 저장해놓고 입력에 대한 결과를 호출만함\n",
    "    \n",
    "##### 미분\n",
    "* x값에 대한 y값의 순간 변화율(순간 기울기)\n",
    "    - 변화율를 측정하기 위해 '미분'함\n",
    "\n",
    "* 수치 미분은 단순하지만 계산이 오래 걸림 -> 효율적인 방법은?\n",
    "* Backpropagation(오차역전파법) :가중치 매개변수의 기울기를 효융적을 계산\n",
    "    - 오차를 역방햑으로 전파하는 방법\n",
    "\n",
    "사과| *2| *1.1 |-> 장바구니\n",
    "---|---|---|---\n",
    "100| 200| 220 |-> 220\n",
    "90| (<-2.2)180| (<-1.1)198| <- 1\n",
    "* 사과에 가격이 장바구니에 2.2배의 영향을 줌\n",
    "* 참고 : week3.pdf - p15\n",
    "\n",
    "* 덧셈 노드\n",
    "    - 그대로 역으로 진행\n",
    "* 곱셈 노드\n",
    "    - x와 y값을 반대로(바꿔서) 곱셈 후 역으로 진행\n",
    "\n",
    "## 연쇄 법칙(Chain Rule)\n",
    "* $z = g(f(x))$\n",
    "* ${\\displaystyle {\\frac {dz}{dx}}={\\frac {dz}{dt}}\\cdot {\\frac {dt}{dx}}}$\n",
    "* 역전파 법은 '국소적 미분'을 전달하는 방식\n",
    "* 국소적 미분을 전달하는 원리는 __연쇄법칙__에 따른 것\n",
    "* '수치 미분'으로 전 과정(layer)을 미분하면 복잡하니, 과정(layer)을 나누어 미분하여 계산함\n",
    "\n",
    "#### 결국 '__Cost function__'을 미분해서 반영하기에, '__활성화 함수__' 선택이 중요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제\n",
    "#### W_5의 가중치 update\n",
    "* 참고 : https://wikidocs.net/37406\n",
    "* 참고 : 이미지 노트(원노트)\n",
    "\n",
    "#### W_1의 가중치 update\n",
    "*  ${\\displaystyle {\\frac {d(MSE)}{dW_1}}={\\frac {d(MSE)}{dh_1}}\\cdot {\\frac {dh_1}{dz_1}} \\cdot {\\frac {dz_1}{dW_1}}}$\n",
    "* 결국 '__Cost function__'을 미분해서 반영하기에, '__활성화 함수__' 선택이 중요!\n",
    "    - Loss function : MSE, MAE, binary/categorical/sparse categorical crossentropy 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
